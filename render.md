Variational autoencoder - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Overview of architecture and operation 2 Formulation 3 Evidence lower bound (ELBO) 4 Reparameterization 5 Variations 6 Statistical distance VAE variants 7 See also 8 References 9 Further reading Toggle the table of contents Variational autoencoder 8 languages فارسی Français 한국어 Հայերեն 日本語 Simple English Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Deep learning generative model to encode data representation The basic scheme of a variational autoencoder. The model receives x {\displaystyle x} as input. The encoder compresses it into the latent space. The decoder receives as input the information sampled from the latent space and produces x ′ {\displaystyle {x'}} as similar as possible to x {\displaystyle x} . Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Artificial neural network Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural radiance field Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Q-learning SARSA Temporal difference (TD) Multi-agent Self-play Learning with humans Active learning Crowdsourcing Human-in-the-loop RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Journals and conferences ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning v t e In machine learning , a variational autoencoder ( VAE ) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling . [ 1 ] It is part of the families of probabilistic graphical models and variational Bayesian methods . [ 2 ] In addition to being seen as an autoencoder neural network architecture, variational autoencoders can also be studied within the mathematical formulation of variational Bayesian methods , connecting a neural encoder network to its decoder through a probabilistic latent space (for example, as a multivariate Gaussian distribution ) that corresponds to the parameters of a variational distribution. Thus, the encoder maps each point (such as an image) from a large complex dataset into a distribution within the latent space, rather than to a single point in that space. The decoder has the opposite function, which is to map from the latent space to the input space, again according to a distribution (although in practice, noise is rarely added during the decoding stage). By mapping a point to a distribution instead of a single point, the network can avoid overfitting the training data. Both networks are typically trained together with the usage of the reparameterization trick , although the variance of the noise model can be learned separately. [ citation needed ] Although this type of model was initially designed for unsupervised learning , [ 3 ] [ 4 ] its effectiveness has been proven for semi-supervised learning [ 5 ] [ 6 ] and supervised learning . [ 7 ] Overview of architecture and operation [ edit ] A variational autoencoder is a generative model with a prior and noise distribution respectively. Usually such models are trained using the expectation-maximization meta-algorithm (e.g. probabilistic PCA , (spike & slab) sparse coding). Such a scheme optimizes a lower bound of the data likelihood, which is usually intractable, and in doing so requires the discovery of q-distributions, or variational posteriors . These q-distributions are normally parameterized for each individual data point in a separate optimization process. However, variational autoencoders use a neural network as an amortized approach to jointly optimize across data points. This neural network takes as input the data points themselves, and outputs parameters for the variational distribution. As it maps from a known input space to the low-dimensional latent space, it is called the encoder. The decoder is the second neural network of this model. It is a function that maps from the latent space to the input space, e.g. as the means of the noise distribution. It is possible to use another neural network that maps to the variance, however this can be omitted for simplicity. In such a case, the variance can be optimized with gradient descent. To optimize this model, one needs to know two terms: the "reconstruction error", and the Kullback–Leibler divergence (KL-D). Both terms are derived from the free energy expression of the probabilistic model, and therefore differ depending on the noise distribution and the assumed prior of the data. For example, a standard VAE task such as IMAGENET is typically assumed to have a gaussianly distributed noise; however, tasks such as binarized MNIST require a Bernoulli noise. The KL-D from the free energy expression maximizes the probability mass of the q-distribution that overlaps with the p-distribution, which unfortunately can result in mode-seeking behaviour. The "reconstruction" term is the remainder of the free energy expression, and requires a sampling approximation to compute its expectation value. [ 8 ] More recent approaches replace Kullback–Leibler divergence (KL-D) with various statistical distances , see see section "Statistical distance VAE variants" below. . Formulation [ edit ] From the point of view of probabilistic modeling, one wants to maximize the likelihood of the data x {\displaystyle x} by their chosen parameterized probability distribution p θ ( x ) = p ( x | θ ) {\displaystyle p_{\theta }(x)=p(x|\theta )} . This distribution is usually chosen to be a Gaussian N ( x | μ , σ ) {\displaystyle N(x|\mu ,\sigma )} which is parameterized by μ {\displaystyle \mu } and σ {\displaystyle \sigma } respectively, and as a member of the exponential family it is easy to work with as a noise distribution. Simple distributions are easy enough to maximize, however distributions where a prior is assumed over the latents z {\displaystyle z} results in intractable integrals. Let us find p θ ( x ) {\displaystyle p_{\theta }(x)} via marginalizing over z {\displaystyle z} . p θ ( x ) = ∫ z p θ ( x , z ) d z , {\displaystyle p_{\theta }(x)=\int _{z}p_{\theta }({x,z})\,dz,} where p θ ( x , z ) {\displaystyle p_{\theta }({x,z})} represents the joint distribution under p θ {\displaystyle p_{\theta }} of the observable data x {\displaystyle x} and its latent representation or encoding z {\displaystyle z} . According to the chain rule , the equation can be rewritten as p θ ( x ) = ∫ z p θ ( x | z ) p θ ( z ) d z {\displaystyle p_{\theta }(x)=\int _{z}p_{\theta }({x|z})p_{\theta }(z)\,dz} In the vanilla variational autoencoder, z {\displaystyle z} is usually taken to be a finite-dimensional vector of real numbers, and p θ ( x | z ) {\displaystyle p_{\theta }({x|z})} to be a Gaussian distribution . Then p θ ( x ) {\displaystyle p_{\theta }(x)} is a mixture of Gaussian distributions. It is now possible to define the set of the relationships between the input data and its latent representation as Prior p θ ( z ) {\displaystyle p_{\theta }(z)} Likelihood p θ ( x | z ) {\displaystyle p_{\theta }(x|z)} Posterior p θ ( z | x ) {\displaystyle p_{\theta }(z|x)} Unfortunately, the computation of p θ ( z | x ) {\displaystyle p_{\theta }(z|x)} is expensive and in most cases intractable. To speed up the calculus to make it feasible, it is necessary to introduce a further function to approximate the posterior distribution as q ϕ ( z | x ) ≈ p θ ( z | x ) {\displaystyle q_{\phi }({z|x})\approx p_{\theta }({z|x})} with ϕ {\displaystyle \phi } defined as the set of real values that param

What is a Variational Autoencoder? | IBM Home Think Topics Variational autoencoder What is a variational autoencoder? Explore IBM's AI platform Sign up for AI updates Published: 12 June 2024 Contributors: Dave Bergmann, Cole Stryker What is a variational autoencoder? Variational autoencoders (VAEs) are generative models used in machine learning (ML) to generate new data in the form of variations of the input data they’re trained on. In addition to this, they also perform tasks common to other autoencoders, such as denoising. Like all autoencoders , variational autoencoders are deep learning models composed of an encoder that learns to isolate the important latent variables from training data and a decoder that then uses those latent variables to reconstruct the input data. However, whereas most autoencoder architectures encode a discrete , fixed representation of latent variables, VAEs encode a continuous , probabilistic representation of that latent space. This enables a VAE to not only accurately reconstruct the exact original input, but also use variational inference to generate new data samples that resemble the original input data. The neural network architecture for the variational autoencoder was originally proposed in a 2013 paper by Diederik P. Kingma and Max Welling, titled Auto-Encoding Variational Bayes (link resides outside ibm.com). This paper also popularized what they called the reparameterization trick , an important machine learning technique that enables the use of randomness as a model input without compromising the model’s differentiability—that is, the ability to optimize the model’s parameters. While VAEs are frequently discussed in the context of image generation, including in this article, they can be used for a diverse array of artificial intelligence (AI) applications, from anomaly detection 1 to generating new drug molecules 2 (link resides outside ibm.com). Ebook How to choose the right AI foundation model While most organizations are clear about the outcomes they expect from generative AI, choosing the wrong model can severely impact your business. In this ebook, explore a model selection framework to balance performance requirements with cost, risk, deployment needs and stakeholder requirements. What is latent space? Essential to understanding VAEs or any other type of autoencoders is the notion of latent space , the name given to the collective latent variables of a specific set of input data. In short, latent variables are underlying variables of data that inform the way the data is distributed but are often not directly observable. For a useful visualization of the concept of latent variables, imagine a bridge with a sensor that measures the weight of each passing vehicle. Naturally, there are different kinds of vehicles that use the bridge, from small, lightweight convertibles to huge, heavy trucks. Because there is no camera, we have no way to detect if a specific vehicle is a convertible, sedan, van or truck. However, we do know that the type of vehicle significantly influences that vehicle’s weight. This example thus entails two random variables, x and z , in which x is the directly observable variable of vehicle weight and z is the latent variable of vehicle type. The primary training objective for any autoencoder is for it to learn how to efficiently model the latent space of a particular input. Latent space and dimensionality reduction Autoencoders model latent space through dimensionality reduction : the compression of data into a lower-dimensional space that captures the meaningful information contained in the original input. In a machine learning (ML) context, mathematical dimensions correspond not to the familiar spatial dimensions of the physical world, but to features of data. For example, a 28x28-pixel black-and-white image of a handwritten digit from the MNIST data set can be represented as a 784-dimensional vector, in which each dimension corresponds to an individual pixel whose value ranges from 0 (for black) to 1 (for white). That same image in color might be represented as a 2,352-dimensional vector, in which each of the 784 pixels is represented in three dimensions corresponding to its respective red, green and blue (RGB) values. However, not all those dimensions contain useful information. The actual digit itself represents only a small fraction of the image, so most of the input space is background noise. Compressing data down to only the dimensions containing relevant information—the latent space —can improve the accuracy, efficiency and efficacy of many ML tasks and algorithms. What is an autoencoder? VAEs are a subset of the larger category of autoencoders , a neural network architecture typically used in deep learning for tasks such as data compression, image denoising, anomaly detection and facial recognition. Autoencoders are self-supervised systems whose training goal is to compress (or encode ) input data through dimensionality reduction and then accurately reconstruct (or decode ) their original input by using that compressed representation. On a fundamental level, the function of an autoencoder is to effectively extract the data’s most salient information—its latent variables—and discard irrelevant noise. What distinguishes different types of autoencoders from one another is the specific strategy they employ to extract that information and the use cases to which their respective strategy is best suited. In training, the encoder network passes input data from the training data set through a "bottleneck" before it reaches the decoder. The decoder network, in turn, is then responsible for reconstructing the original input by using only the vector of latent variables. After each training epoch, optimization algorithms such as gradient descent are used to adjust model weights in a way that minimizes the difference between the original data input and the decoder’s output. Eventually, the encoder learns to allow through the information most conducive to accurate reconstruction and the decoder learns to effectively reconstruct it. While this most intuitively lends itself to straightforward data compression tasks, the ability to efficiently encode accurate latent representations of unlabeled data gives autoencoders a wide variety of applications. For example, autoencoders can be used to restore corrupted audio files, colorize grayscale images or detect anomalies (such as those resulting from fraud) that would otherwise be invisible to the naked eye. Autoencoder structure Though different types of autoencoders add or alter certain aspects of their architecture to better suit specific goals and data types, all autoencoders share three key structural elements: The encoder extracts latent variables of input data x and outputs them in the form of a vector representing latent space z. In a typical "vanilla" autoencoder, each subsequent layer of the encoder contains progressively fewer nodes than the previous layer; as data traverses each encoder layer, it’s compressed through the process of “squeezing” itself into fewer dimensions. Other autoencoder variants instead use regularization terms, like a function that enforces sparsity by penalizing the number of nodes that are activated at each layer, to achieve this dimensionality reduction. The bottleneck , or "code," is both the output layer of the encoder network and the input layer of the decoder network. It contains the latent space: the fully compressed, lower-dimensional embedding of the input data. A sufficient bottleneck is necessary to help ensure that the decoder cannot simply copy or memorize the input data, which would nominally satisfy its training task but prevent the autoencoder from learning. The decoder uses that latent representation to reconstruct the original input by essentially reversing the encoder: in a typical decoder architecture, each subsequent layer contains a progressively larger number of active nodes. While the encoder and decoder networks of many autoencoders are built from standard multilayer perceptrons (MLPs), autoencoders are not confined to any specific type of neural network. Autoencoders used for computer vision tasks are often convolutional neural networks (CNNs) and are thus called convolutional autoencoders. Autoencoders built from transformer architecture have been used in multiple fields, including computer vision 3 and music. 4 A key benefit of autoencoders over other dimensionality reduction algorithms, such as principal component analysis (PCA), is that autoencoders can model nonlinear relationships between different variables. For that reason, the nodes of autoencoder neural networks typically use nonlinear activation functions. In many autoencoder applications, the decoder serves only to aid in the optimization of the encoder and is thus discarded after training. In variational autoencoders, the decoder is retained and used to generate new data points. How do variational autoencoders work? What distinguishes VAEs from other autoencoders is the unique way they encode latent space and the different use cases to which their probabilistic encoding can be applied. Unlike most autoencoders, which are deterministic models that encode a single vector of discrete latent variables, VAES are probabilistic models. VAEs encode latent variables of training data not as a fixed discrete value z , but as a continuous range of possibilities expressed as a probability distribution p ( z ) . In Bayesian statistics , this learned range of possibilities for the latent variable is called the prior distribution . In variational inference , the generative process of synthesizing new data points, this prior distribution is used to calculate the posterior distribution , p ( z | x ). In other words, the value of observable variables x, given a value for latent variable z . For each latent attribute of training data, VAEs encode two different latent vectors: a vector of means, “ μ ,” and a v

[1906.02691] An Introduction to Variational Autoencoders Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1906.02691 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Machine Learning arXiv:1906.02691 (cs) [Submitted on 6 Jun 2019 ( v1 ), last revised 11 Dec 2019 (this version, v3)] Title: An Introduction to Variational Autoencoders Authors: Diederik P. Kingma , Max Welling View a PDF of the paper titled An Introduction to Variational Autoencoders, by Diederik P. Kingma and Max Welling View PDF Abstract: Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions. Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML) Cite as: arXiv:1906.02691 [cs.LG] (or arXiv:1906.02691v3 [cs.LG] for this version) https://doi.org/10.48550/arXiv.1906.02691 Focus to learn more arXiv-issued DOI via DataCite Journal reference: Foundations and Trends in Machine Learning: Vol. 12 (2019): No. 4, pp 307-392 Related DOI : https://doi.org/10.1561/2200000056 Focus to learn more DOI(s) linking to related resources Submission history From: Diederik P. Kingma Dr. [ view email ] [v1] Thu, 6 Jun 2019 16:35:38 UTC (2,905 KB) [v2] Wed, 24 Jul 2019 05:44:14 UTC (2,904 KB) [v3] Wed, 11 Dec 2019 17:33:13 UTC (1,119 KB) Full-text links: Access Paper: View a PDF of the paper titled An Introduction to Variational Autoencoders, by Diederik P. Kingma and Max Welling View PDF TeX Source Other Formats view license Current browse context: cs.LG < prev | next > new | recent | 2019-06 Change to browse by: cs stat stat.ML References & Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Diederik P. Kingma Max Welling a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack

[1606.05908] Tutorial on Variational Autoencoders Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > stat > arXiv:1606.05908 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Statistics > Machine Learning arXiv:1606.05908 (stat) [Submitted on 19 Jun 2016 ( v1 ), last revised 3 Jan 2021 (this version, v3)] Title: Tutorial on Variational Autoencoders Authors: Carl Doersch View a PDF of the paper titled Tutorial on Variational Autoencoders, by Carl Doersch View PDF Abstract: In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed. Subjects: Machine Learning (stat.ML) ; Machine Learning (cs.LG) Cite as: arXiv:1606.05908 [stat.ML] (or arXiv:1606.05908v3 [stat.ML] for this version) https://doi.org/10.48550/arXiv.1606.05908 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Carl Doersch [ view email ] [v1] Sun, 19 Jun 2016 21:02:30 UTC (617 KB) [v2] Sat, 13 Aug 2016 12:33:43 UTC (600 KB) [v3] Sun, 3 Jan 2021 16:56:46 UTC (600 KB) Full-text links: Access Paper: View a PDF of the paper titled Tutorial on Variational Autoencoders, by Carl Doersch View PDF TeX Source Other Formats view license Current browse context: stat.ML < prev | next > new | recent | 2016-06 Change to browse by: cs cs.LG stat References & Citations NASA ADS Google Scholar Semantic Scholar 6 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack

An introduction to Variational Auto Encoders (VAEs) | by Emrick Sinitambirivoutin | Towards Data Science Open in app Sign up Sign in Write Sign up Sign in An introduction to Variational Auto Encoders (VAEs) Understanding Variational Autoencoders (VAEs) from theory to practice using PyTorch Emrick Sinitambirivoutin · Follow Published in Towards Data Science · 10 min read · Jun 17, 2020 -- 3 Listen Share Art work from https://joanielemercier.com/ (Eyjafjallajökull, NY, May 2010 — commissioned by onedotzero) VAE are latent variable models [1,2]. Such models rely on the idea that the data generated by a model can be parametrized by some variables that will generate some specific characteristics of a given data point. These variables are called latent variables. One of the key ideas behind VAE is that instead of trying to construct a latent space (space of latent variables) explicitly and to sample from it in order to find samples that could actually generate proper outputs (as close as possible to our distribution), we construct an Encoder-Decoder like network which is split in two parts: The encoder learns to generate a distribution depending on input samples X from which we can sample a latent variable that is highly likely to generate X samples. In other words we learn a set of parameters θ1 that generate a distribution Q(X,θ1) from which we can sample a latent variable z maximizing P(X|z). The decoder part learns to generate an output which belongs to the real data distribution given a latent variable z as an input. In other words, we learn a set of parameters θ2 that generates a function f(z,θ2) that maps the latent distribution that we learned to the real data distribution of the dataset. Variational Auto Encoder global architecture In order to understand the mathematics behind Variational Auto Encoders, we will go through the theory and see why these models works better than older approaches. This article will cover the following How to define the construct the latent space How to generate data efficiently from latent space sampling. The final architecture of VAEs Some experiments showing interesting properties of VAEs 1. Latent Variable Models Latent variable models come from the idea that the data generated by a model needs to be parametrized by latent variables. This name comes from the fact that given just a data point produced by the model, we don’t necessarily know which settings of the latent variables generated this data point. In a more formal setting, we have a vector of latent variables z in a high-dimensional space Z which we can easily sample according to some probability density function P ( z ) defined over Z. Then, we have a family of deterministic functions f ( z ; θ ), parameterized by a vector θ in some space Θ, where f :Z×Θ→X. f is deterministic, but if z is random and θ is fixed, then f ( z ; θ ) is a random variable in the space X . During training, we optimize θ such that we can sample z from P ( z ) and, with high probability, having f ( z ; θ ) as close as the X ’s in the dataset. In order to achieve that, we need to find the parameters θ such that: Here, we just replace f ( z ; θ ) by a distribution P ( X | z ; θ ) in order to make the dependence of X on z explicit by using the law of total probability. An other assumption that we make is to suppose that P(W|z; θ ) follow a Gaussian distribution N(X| f ( z ; θ ), σ*I) (By doing so we consider that generated data are almost as X but not exactly X). Defining the latent space As explained in the beginning, the latent space is supposed to model a space of variables influencing some specific characteristics of our data distribution. We can imagine that if the dataset that we consider is composed of cars and that our data distribution is then the space of all possible cars, some components of our latent vector would influence the color, the orientation or the number of doors of a car. However, it is rapidly very tricky to explicitly define the role of each latent components, particularly when we are dealing with hundreds of dimensions. In addition to that, some component can depends on others which makes it even more complex to design by hand this latent space. In other words, it’s really difficult to define this complex distribution P(z). Solution In order to overcome this issue, the trick is to use a mathematical property of probability distributions and the ability of neural networks to learn some deterministic functions under some constrains with backpropagation. The mathematical property that makes the problem way more tractable is that: Any distribution in d dimensions can be generated by taking a set of d variables that are normally distributed and mapping them through a sufficiently complicated function. As a consequence, we can arbitrarily decide our latent variables to be Gaussians and then construct a deterministic function that will map our Gaussian latent space into the complex distribution from which we will sample to generate our data. The deterministic function needed to map our simple latent distribution into a more complex one that would represent our complex latent space can then be build using a neural network with some parameters that can be fine tuned during training. 2. Learn to generate data from the latent space Before jumping into the interesting part of this article, let’s recall our final goal: We have a d dimensional latent space which is normally distributed and we want to learn a function f(z;θ2) that will map our latent distribution to our real data distribution. In other words we want to sample latent variables and then use this latent variable as an input of our generator in order to generate a data sample that will be as close as possible of a real data points. We still need to resolve two things: How do we explore our latent space efficiently in order to discover the z that will maximize the probability P(X|z)? (we need to find the right z for a given X during training) How do we train this all process using back propagation? (we need to find an objective that will optimize f to map P(z) to P(X)) Finding the right z latent variable for our X data sample In practice, for most z , P (X|z) will be nearly zero, and hence contribute almost nothing to our estimate of P (X). The key idea behind the variational auto-encoder is to attempt to sample values of z that are likely to have produced X, and compute P(X) just from those. In order to do that, we need a new function Q (z|X) which can take a value of X and give us a distribution over z values that are likely to produce X. Hopefully the space of z values that are likely under Q will be much smaller than the space of all z’s that are likely under the prior P(z). This part of the VAE will be the encoder and we will assume that Q will be learned during training by a neural network mapping the input X to the output Q(z|X) which will be the distribution from which we are most likely to find a good z to generate this particular X. Training the model with backpropagation In order to understand how to train our VAE, we first need to define what should be the objective, and to do so, we will first need to do a little bit of maths. Let’s start with the Encoder, we want Q(z|X) to be as close as possible to P(X|z). In order to measure how close the two distributions are, we can use the Kullback-Leibler divergence D between the two distributions: With a little bit of maths, we can rewrite this equality in a more interesting way. By applying the Bayes rule on P(z|X) we have: Which is equivalent to: Let’s take a time to look at this formulae Part A : The left term is not really interesting for our backpropagation setting (we don’t know a simple expression for P(X)), but log(P(X)) is actually what we want to maximize given a z and we can see here that we can do so by minimizing the right part (making Q(z|X) as close as possible to P(z|X)). This is exactly what we mentioned at the beginning. Part B : This term is much more interesting as we know P(X|z) (it’s our decoder part -> generator) and Q(z|X) (it’s our encoder). We can see here that in order to maximize this term, we need to maximize log(P(X|z)) which means that we wan’t to maximize the log likelihood of our probability and minimize the KL Divergence between Q(z|X) and P(z). In order to make Part B more easy to compute is to suppose that Q(z|X) is a gaussian distribution N(z|mu(X,θ1), sigma(X,θ1)) where θ1 are the parameters learned by our neural network from our data set. One issue remains unclear with our formulae : How do we compute the expectation during backpropagation ? Handling the expectation operator One way would be to do multiple forward pass in order to be able to compute the expectation of the log(P(X|z)) but this is computationally inefficient. Hopefully, as we are in a stochastic training, we can supposed that the data sample Xi that we we use during the epoch is representative of the entire dataset and thus it is reasonable to consider that the log(P(Xi|zi)) that we obtain from this sample Xi and the dependently generated zi is representative of the expectation over Q of log(P(X|z)). Finally, the decoder is simply a generator model that we want to reconstruct the input image so a simple approach is to use the mean square error between the input image and the generated image. 3. Final architecture of VAEs We can know resume the final architecture of a VAE. As announced in the introduction, the network is split in two parts: The encoder that learns to generate a distribution depending on input samples X from which we can sample a latent variable that is highly likely to generate X samples. This part needs to be optimized in order to enforce our Q(z|X) to be gaussian. The decoder part learns to generate an output which belongs to the real data distribution given a latent variable z as an input. This part maps a sampled z (initially from a normal distribution) into a more complex latent space (the one ac

%PDF-1.5
%�
7 0 obj
<< /Type /XObject /Subtype /Form /BBox [ 0 0 105 33 ]
/Filter /FlateDecode /FormType 1 /Length 1314
/PTEX.FileName (./now_logo.pdf) /PTEX.InfoDict 21 0 R
/PTEX.PageNumber 1
/Resources << /ExtGState << /R7 22 0 R >> /ProcSet [ /PDF /ImageC ]
/XObject << /R10 24 0 R /R11 23 0 R /R8 26 0 R /R9 25 0 R >> >> >>
stream
x�Ŗ=�d7��w
��n����ɹ������xJq��<-�桔�l��J1y	oW��v�������Z�-�4��l T����e�Xz�sn`q-���I=����"�;O�pc��<`�_�3���g6\��)e�J�[U}��6�������cJL=њ�qV��x�9���VʱU)�\�s�m�9ǩ�FG�䅽�� ���hy��]I>����vlρ������W7�����
z��u�R��k9�z4�S��c���oW��c>=P.&J	�l�Bm�HC�HRs��j��*�V$�U
#d��N�k=�9볿��L� m�C(������� �v�8�\&�s]�ȴ�$&j�)qAQ\r4M
kvr�����6�鉞����sO1��Dr/�#d�c� 4�$��N]tWs�����շ�	��о_z���0��7//ݝip�i�z�}׹w�Q��kn>_s�;�G�V�y����!�Rb��m�yp�$�<�|7eG@����������xA>"���=;�ag���������9�֖�dYםR�X=����H��1I�2��͐Һ鷇|�m���Dn���m�=����hSo�}�؞�Ď�����+lޅ�r�Ӛ�=endstream
endobj
23 0 obj
<< /Subtype /Image /BitsPerComponent 8 /ColorSpace /DeviceRGB
/DecodeParms << /Colors 3 /Columns 437 /Predictor 15 >>
/Filter /FlateDecode /Height 136 /Length 859 /Width 437 >>
stream
x��ԱM A,��J���Dp��g��`�}| py�: �M�#��np�G��?���� 7��#��np�G��?���� 7��#��np�G��?����c>�^���߿�N����1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#�8��)��1��H�?����G
�q�?�?R��c�1����������c�#p�G��?���� 7��#��np�G��?���� 7��#��np�G��?���� 7��#��n��endstream
endobj
24 0 obj
<< /Subtype /Image /BitsPerComponent 8 /ColorSpace /DeviceRGB
/DecodeParms << /Colors 3 /Columns 437 /Predictor 15 >>
/Filter /FlateDecode /Height 136 /Length 949 /Width 437 >>
stream
x���AM DA�@�l0�M�%3
��=> �<^ ���� 7��#��np�G��?���� 7��#��np��1�?_�Nxk���W'������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3����c������"㏙?R�c�1�G��q�?f�H�?����)��1���#E�8�3��np�G��?���� 7��#��np�G��?���� 7��#���v8�endstream
endobj
25 0 obj
<< /Subtype /Image /BitsPerComponent 8 /ColorSpace /DeviceRGB
/DecodeParms << /Colors 3 /Columns 437 /Predictor 15 >>
/Filter /FlateDecode /Height 136 /Length 453 /Width 437 >>
stream
x���1 0��տ��o�D� (� p�?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G�� ��?4h����#@�G��D� �endstream
endobj
26 0 obj
<< /Subtype /Image /BitsPerComponent 8 /ColorSpace /DeviceRGB
/DecodeParms << /Colors 3 /Columns 437 /Predictor 15 >>
/Filter /FlateDecode /Height 136 /Length 434 /Width 437 >>
stream
x��� ���?��"fw ��#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@�#@;�ė endstream
endobj
8 0 obj
<< /Type /XObject /Subtype /Form /BBox [ 0 0 162 16 ]
/Filter /FlateDecode /FormType 1 /Length 6028
/PTEX.FileName (./essence_logo.pdf) /PTEX.InfoDict 27 0 R
/PTEX.PageNumber 1
/Resources << /ExtGState << /R7 28 0 R >> /ProcSet [ /PDF /ImageC ]
/XObject << /R8 30 0 R /R9 29 0 R >> >> >>
stream
�����ހ=�j�=���C�pDP��YS����|$�A)����w������������_��ۊ=6�����zY�����u���_��}���8�
��>U���?/3��H�ڈ�����n�I�s���c�!`q�k��qO�f��^�^J�s-`g|��$.{�=�,�'� ��8�#b�b{��n��R���G����:��u��`��֟)_����k�V��j���$�T8��}�OA0.	�aO��'7�))XG���X�XI|���ƾ
���P����\�J��j?,4麆���BC��i�P-w�=�*e)�\���Lh	׻�,���~q�a,0�b�
��F� :���z�� �%:+xvL��o�c����ތ����a�xp�%PU0|��q:�+�]v_���2�\[�
��=�W��a�t$-��ʘ��{QYn�}��`ht#�T�4;6H�b@����t��w8l�U��`���u���f�-��b?0�8څA,�����o�E (����a�}��a��w<�0��׾1�L!d�k�B�l�@�$����a27�������BN�Fa��͹\(�*nRyT�Ba��Aa�t�`
c)(\��%Y(��S
�0"N�(���B�s/���'�0��0�f�f�� e�	����a�>`�i��!�K?(��}�0��bc\	
l�B��Dab�E 8(�� y�O�7_(��,�0���hL6g�
o�҂Z~7l�ػ>�S6�RLS��<�HRH25��	�
o��4JZ�6�J�pM�.�@"i�:���j�Qל
��S�c���q�+�ޚ���9�j�d%��gA  ,KӘ�pj�����<�\���4{��1j���*ȩ��w�k��(&���hVG8�����G�e,��{�An	Wǋ��i�t�|M��yA�JGL!�U3��L�Rڬ6�_ǶY�V�$��f� ��j3�/=�a��A&��$G;����Ej�lz�%�~:��R/�����/����,�-R[�P������x�Y�7K$��D���kK�27�ͣ��/֪��~��v6�:klm�Z�3չh��=�Mk��	���9ym��F����PX�Y�~������.�t��G	���d0
�,Dß�"6u����^�^��!�%7:�K����	-��WU��W2�6ԹP���_�<�ؒN�(��9�Ǯ5�,:�A��ז\3#���Z7�06��J�\ܒ��}�TG���y�p ��������9(A�+�̒� ��bvE�TS�^�o|������im�t�nw�uZ�m`�xu���I� o�L�1&�^	2B�T�2��!c�&���BG�5Г!CB�u_�"�ΐ���G�deȐ����v�~5v���'C^�k�I�k���d�K�2d0@��c�O���`��ΐ1~����/�\2���.�z�N����_	2'�0��C����N���j����j9��c�zs1cNM�����k��M ���2�~�Z�&c��Eưb�����L�6[��]R6�8�M�4��"c.���2�1���i��fc���ƴЫ�gL����ɞ+�0�v����q�1��	������Y�}�[���0���&d߷�܌%!�$>ޓ!����g��-3�-/F�	��s<�훑��\���mF�1#�fd{|��K#�T�qF�w�E� ���&d�d��ni7�:/�=�8j�1i�����5���c��yV��,%𭍞�[8ӳă����V�cK����S1������lO'�^^����L	�����W�;��4��#h<����� ��I����n@�4?O���,��죏�QR;��>�]?%��u�b��D���n	Ch��<��G20�'�A��ٚ�K��gr�d��8����!�gL�z�q�L�~��:]?\�*�V��ߐ(��{�b$��U7m�����<�N�*��8"$�G��V�����w�
��:_v �926��q.V�A۴���+H'8V�.]f5���VڍBO��bS�e���q��t�$�BifKg�ұcbA�H�\�,p(��J}��j��v�3qv.�k��llv/=�t龀�b:�%|0�Oǃ�!�Q�|���L���X�NDsI��V�46lܼjJ6�4�c��+N{�?w�L^�K���%�0�|�V���9�XL;]�	pϧK���v-����� ]k�k5y���ZM�/�k�=�-b�~�|	5�$�*_.�*_[E�֫�%�,�m�|K�o��<�Jc�X������|����)_�Y���n[eW��V#%���9��V�d���	�������ݵʶ�z��4�zj�Zƨ7�@T�r����O�*�"2���ƶ�q��P%�v�cy«kU=y�n[�f��NN���q5�V��rb��t��Z�Ec�_�� �:G�n���[����V5����� �}*�늨��N
?s��6���<��?����_�Ô��=��oޢ����[���$��c�ܞ�a�l6KW���a��Azi�NJӷ;}(�����wc"f������>�?�ğ�|u$B���\K��7�1�˛�X�Ih�,�������GoDC�>z۳\_�����K����������sY������0�����*�endstream
endobj
29 0 obj
<< /Subtype /Image /BitsPerComponent 8 /ColorSpace /DeviceRGB
/DecodeParms << /Colors 3 /Columns 672 /Predictor 15 >>
/Filter /FlateDecode /Height 63 /Length 575 /Width 672 >>
stream
�/1x���������_3���<~��K2��/1x ��w���� ��1���~��K2��/1x ��w���� ��1����I��a�d�@����W`�@������3��x|��q�Ǉ<g��y|��q�#�O2x ����������1x ��O�����$��1x ��������<�/1x ���3����<�/1x���G����,w�2����}��Kh2��/1x ��w���� ��1��j�}��K�1�5�/1x��������w�

https://en.wikipedia.org/wiki/Variational_autoencoder
https://www.ibm.com/think/topics/variational-autoencoder
https://arxiv.org/abs/1906.02691
https://arxiv.org/abs/1606.05908
https://www.datacamp.com/tutorial/variational-autoencoders
https://towardsdatascience.com/an-introduction-to-variational-auto-encoders-vaes-803ddfb623df
https://arxiv.org/pdf/1906.02691
